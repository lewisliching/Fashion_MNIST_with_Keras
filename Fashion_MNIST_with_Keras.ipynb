{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion MNIST with Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/lewisliching/Fashion_MNIST_with_Keras/blob/master/Fashion_MNIST_with_Keras.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "CXJyomi85Frq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Fashion MNIST with Keras for getting start of Google Colab\n",
        "\n",
        "- Take Fashion MNIST with Keras as example of getting start Deep learning in Google Colab and Google Drive\n",
        "\n",
        "- Each Fashion image is 28 x 28 = 784 pixels in total.\n",
        "- Each pixel value is an integer between 0 and 255 with higher for darker.\n",
        "- In the csv dataset, each row is consist of  1 columns(1st column as label) and 784 columns (as 784 pixels of one image)\n",
        "- labels of 1st column is as follows:\n",
        "  0.\tT-shirt/top\n",
        "  1.\tTrouser\n",
        "  2.\tPullover\n",
        "  3.\tDress\n",
        "  4.\tCoat\n",
        "  5.\tSandal\n",
        "  6.\tShirt\n",
        "  7.\tSneaker\n",
        "  8.\tBag\n",
        "  9.\tAnkle boot\n",
        "\n",
        "- 60000 images in fashion-mnist_train.csv\n",
        "- 10000 images in fashion-mnist_test.csv\n",
        "\n",
        "\n",
        "- Reference:\n",
        "\n",
        " - https://www.kaggle.com/zalando-research/fashionmnist/home\n",
        " - https://github.com/zalandoresearch/fashion-mnist"
      ]
    },
    {
      "metadata": {
        "id": "l8foMygEsQjE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Set up Working environemnt of Google Colab and Google drive\n",
        "\n",
        "- Supposed Tensorflow is ready under Googel Colab\n",
        "\n",
        "- Don't forget to set Python 3 and using GPU as accelerator in Google Colab\n",
        "\n",
        "- Install Keras, in which Numpy....libraries are updated as well\n",
        "\n",
        "- Install PyDrive for accessing Google drive and authorize Google Colab to access files"
      ]
    },
    {
      "metadata": {
        "id": "Jftu2XXm9asO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "!pip install -U -q PyDrive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ktvlBAbv9BoG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Setup ngrok and run TensorBoard on Colab (optional)\n",
        "\n",
        "**ngrok is service to tunnel traffic from Tensorboard to localhost.**\n",
        "\n",
        "![alt text](https://gitcdn.xyz/cdn/Tony607/blog_statics/d425c3fe4cf0d92067572e25ae6cc3198d51936b//images/ngrok/ngrok.jpg)\n",
        "\n",
        "- Download and unzip ngrok for installation\n",
        "\n",
        "- Set folder \"log\" in current folder of Colab to be the folder of Tensorboard log data\n",
        "\n",
        "- set Tensorboard 6006 port to \"log\" folder and then ngrok service\n",
        "\n",
        "---\n",
        "**Notes:**\n",
        "\n",
        "- Before compiling cnn_model.fit, we need to set up tensorboard to output log data to \"log\" folder\n",
        "\n",
        "-  After compiling, generate Public URL of ngrok to access Tensorboard\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Kf2LKzBB89ZL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uTzMIVHboWvF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Connect Google drive for CSV files\n",
        "\n",
        "- Authenticate and create the PyDrive client and it will prompt for unique key of verify Google Colab can have access to Google Drive. Just click and copy the unique key and copy to the blank\n",
        "\n",
        "- Copy the Folder ID from URL bar of that folder\n",
        "\n",
        "- Search for 2 csv files in the folder and load files into Google colab and create filename accordingly"
      ]
    },
    {
      "metadata": {
        "id": "7Y7ypsOG-FX-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# list out files in folder of Google drive\n",
        "file_list = drive.ListFile({'q': \"'1xt6pBbi0fLeFeUqc4OuwD0SciUad9jhf' in parents and trashed=false\"}).GetList()\n",
        "\n",
        "# Load files into Google colab\n",
        "for file1 in file_list:\n",
        "    if file1['title'] == 'fashion-mnist_train.csv':\n",
        "      train_downloaded = drive.CreateFile({'id': file1['id']})\n",
        "      train_downloaded.GetContentFile(file1['title'])  \n",
        "      print('%s is loaded into Colab' % (file1['title']))\n",
        "\n",
        "    if file1['title'] == 'fashion-mnist_test.csv':\n",
        "      train_downloaded = drive.CreateFile({'id': file1['id']})\n",
        "      train_downloaded.GetContentFile(file1['title'])  \n",
        "      print('%s is loaded into Colab' % (file1['title']))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vWOnYbiUVcPb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Import all relevant libraries & Packages"
      ]
    },
    {
      "metadata": {
        "id": "ojPqTdXJVVaP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WcZRS96foOBO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Import CSV file into DataFrame(Pandas) from Colab\n",
        "\n",
        "- Using Pandas read_csv to read file in raw format\n",
        "\n",
        "- Printout header (1st 5 row) of DataFrame(df) for double checking (1st column label = 2 9 6 0 3)"
      ]
    },
    {
      "metadata": {
        "id": "hyEHaWcgoEVA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(r'fashion-mnist_train.csv')\n",
        "test_df = pd.read_csv(r'fashion-mnist_test.csv')\n",
        "\n",
        "# check df by printout 1st 5 rows\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sAWDDZvpdwHc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load DataFrame (Pandas) to Array (Numpy) for further data analysis\n",
        "\n",
        "- A Series object in pandas represents a one-dimensional labeled indexed array based on the NumPy ndarray. \n",
        "\n",
        "- NumPy allows us to work with high-performance N-dimensional array for selecting array elements, logical operations, slicing, reshaping, combining(stacking), splitting...etc.\n",
        "\n",
        "- A series object is more efficient than using  N-dimensional for aligning data or label matching while N-dimensional array is more efficient in data manipulation. Thus, this is the reason why we load data from Data Frame to Array with:\n",
        "\n",
        "  1. Data Splitting (X & Y for both train data and test data)\n",
        "  2. Data Normalization by 255 (range of 0 - 255 integer of each pixel)\n",
        "  3. Print out dimension of arrays for double checking\n",
        "  4. Print out one of image of x_train for double checking (any number e.g. 50000)"
      ]
    },
    {
      "metadata": {
        "id": "7Zr5kWSkdhMf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = np.array(train_df, dtype='float32')\n",
        "test_data = np.array(test_df, dtype='float32')\n",
        "\n",
        "# all rows from column 1 till end\n",
        "# all rows of column 0\n",
        "x_train = train_data[:, 1:]/255\n",
        "y_train = train_data[:, 0]\n",
        "\n",
        "x_test = test_data[:, 1:]/255\n",
        "y_test = test_data[:, 0]\n",
        "\n",
        "print('x_train:', x_train.shape)\n",
        "print('y_train:', y_train.shape)\n",
        "print(' x_test:', x_test.shape)\n",
        "print(' y_test:', y_test.shape)\n",
        "\n",
        "#take 50000th row data of x_train and reshape into 28x28 array\n",
        "image = x_train[50000, :].reshape(28,28)\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P3rCiml6rTow",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Splitting train data into training data and validate date for model training\n",
        "\n",
        "- From library sklearn, we use train_test_split for data splitting\n",
        "  - test size = 0.2 is meant splitting validate data by 20% of train data\n",
        "  - random_state = 123 (or any number) is keep consistent result of same randomize picking. If we want to true random picking everytime, just don't set this value"
      ]
    },
    {
      "metadata": {
        "id": "4SlePIBlmoL3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "x_training, x_validate, y_training, y_validate = train_test_split(\n",
        "  x_train, y_train, test_size=0.2, random_state=123, \n",
        ")\n",
        "print('x_training:{}'.format(x_training.shape))\n",
        "print('x_validate:{}'.format(x_validate.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UXXWxfwCW1Xs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Create CNN model for deep learning\n",
        "\n",
        "- **Reshape dataset**\n",
        "  - adding 28 x 28 x 1 into the dataset in order to fit CNN model\n",
        "  - Print out the shape of each dataset for double checking\n",
        "  - batch size ?? (not sure, under studying)\n",
        "  \n",
        "- **Define the CNN model**\n",
        " - setting all parameters (under studying)\n",
        " \n",
        "- **Compile the CNN model**\n",
        " - setting all parameters (under studying)\n",
        " \n",
        "- **Setting Tensorboard for log data analysis (optional)**\n",
        " - Set up tensorboard to output log data to \"log\" folder\n",
        " - For details, please refer to the top of this project\n",
        "\n",
        "- **Fit the CNN model**\n",
        " - setting all parameters (under studying)\n",
        " - It really takes a long long time with log data of Tensorboard\n",
        " - Callbacks[tensorboard] is needed only if you want Tensorboard\n",
        "\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "OvU8lMrhW1kN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "im_rows = 28\n",
        "im_cols = 28\n",
        "batch_size = 512\n",
        "im_shape = (im_rows, im_cols, 1)\n",
        "\n",
        "x_training = x_training.reshape(x_training.shape[0], *im_shape)\n",
        "x_validate = x_validate.reshape(x_validate.shape[0], *im_shape)\n",
        "x_test = x_test.reshape(x_test.shape[0], *im_shape)\n",
        "\n",
        "print('x_training shape:{}'.format(x_training.shape))\n",
        "print('x_validate shape:{}'.format(x_validate.shape))\n",
        "print('x_test shape:{}'.format(x_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pofb_dVEa6sO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_model = Sequential([\n",
        "    Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=im_shape),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Dropout(0.2),\n",
        "    \n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_k1eLXbcf0Dt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnn_model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=Adam(lr=0.001),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7EePg8evgJMu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#optional for Tensorboard\n",
        "tensorboard = TensorBoard(log_dir='./log', histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_grads=True,\n",
        "                         batch_size=batch_size,\n",
        "                         write_images=True)\n",
        "\n",
        "cnn_model.fit(\n",
        "    x_training, y_training, batch_size=batch_size,\n",
        "    epochs=10, verbose=1,\n",
        "    validation_data=(x_validate, y_validate),\n",
        "    callbacks=[tensorboard] #optional for Tensorboard\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v9WtNV9UEvG_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Evaluation of the CNN model with test dataset\n",
        "- Evaluate the CNN model by test dataset.\n",
        "- Printout loss & accuracy for evaluation by comparing with fitting result above"
      ]
    },
    {
      "metadata": {
        "id": "M6ITPbSKhl6O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score = cnn_model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('test loss: {:.4f}'.format(score[0]))\n",
        "print('test accuracy: {:.4f}'.format(score[1]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-rpcpGMlFWok",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Access Tensorboard webpage for log data analysis (Optional)\n",
        "- URL of Ngrok will be resulted. Click it for Tensorboard.\n",
        "- For details, please refer to the top of this project."
      ]
    },
    {
      "metadata": {
        "id": "4RMPxo_5nWIK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}